#!/usr/bin/env python3
"""
å¤šè¿›ç¨‹Parquetè½¬JSONLè½¬æ¢å™¨
æ”¯æŒæ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰Parquetæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶è½¬æ¢ä¸ºå¯¹åº”çš„JSONLæ–‡ä»¶
"""

import pandas as pd
import json
import multiprocessing as mp
from pathlib import Path
import time
import argparse
import logging
from typing import List, Dict, Any
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def process_single_file(file_info: tuple) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªParquetæ–‡ä»¶å¹¶è½¬æ¢ä¸ºJSONLæ ¼å¼
    
    Args:
        file_info: å…ƒç»„åŒ…å« (è¾“å…¥æ–‡ä»¶è·¯å¾„, è¾“å‡ºç›®å½•è·¯å¾„)
    
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    input_file_path, output_dir = file_info
    start_time = time.time()
    
    try:
        input_path = Path(input_file_path)
        output_path = Path(output_dir) / f"{input_path.stem}.jsonl"
        
        logger.info(f"å¼€å§‹å¤„ç†: {input_path.name}")
        
        # è¯»å–Parquetæ–‡ä»¶
        df = pd.read_parquet(input_path)
        total_rows = len(df)
        
        # è½¬æ¢ä¸ºJSONLå¹¶å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            for idx, row in df.iterrows():
                # å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå­—å…¸
                row_dict = row.to_dict()
                
                # è½¬æ¢æ¯ä¸ªæ ·æœ¬çš„æ•°æ®
                cleaned_dict = row_dict
                cleaned_dict["problem"] = row_dict['messages'][0]['content']
                cleaned_dict["cot"] = row_dict['messages'][1]['content']
                cleaned_dict['ground_truth'] = json.loads(row_dict['metadata'])["expected_answer"]
                cleaned_dict['problem_source'] = json.loads(row_dict['metadata'])["problem_source"]
                cleaned_dict.pop('messages')
                cleaned_dict.pop('metadata')
                
                # è½¬æ¢ä¸ºJSONå¹¶å†™å…¥
                json_line = json.dumps(cleaned_dict, ensure_ascii=False)
                f.write(json_line + '\n')
                
                # æ¯å¤„ç†10000è¡Œæ‰“å°è¿›åº¦
                if (idx + 1) % 10000 == 0:
                    logger.info(f"{input_path.name}: å·²å¤„ç† {idx + 1}/{total_rows} è¡Œ")
        
        processing_time = time.time() - start_time
        logger.info(f"å®Œæˆ: {input_path.name} -> {total_rows}è¡Œ, è€—æ—¶: {processing_time:.2f}s")
        
        return {
            'input_file': str(input_path),
            'output_file': str(output_path),
            'rows_processed': total_rows,
            'success': True,
            'processing_time': processing_time,
            'error': None
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"å¤„ç†æ–‡ä»¶ {input_file_path} æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        
        # æ¸…ç†å¯èƒ½ç”Ÿæˆçš„ä¸å®Œæ•´æ–‡ä»¶
        if 'output_path' in locals() and output_path.exists():
            try:
                output_path.unlink()
                logger.info(f"å·²åˆ é™¤ä¸å®Œæ•´æ–‡ä»¶: {output_path}")
            except:
                pass
        
        return {
            'input_file': str(input_file_path),
            'output_file': None,
            'rows_processed': 0,
            'success': False,
            'processing_time': processing_time,
            'error': str(e)
        }

def find_parquet_files(input_dir: str) -> List[Path]:
    """
    æŸ¥æ‰¾ç›®å½•ä¸­çš„æ‰€æœ‰Parquetæ–‡ä»¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        
    Returns:
        Parquetæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    input_path = Path(input_dir)
    if not input_path.exists():
        raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰
    parquet_files = list(input_path.rglob("*.parquet"))
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œå°è¯•å…¶ä»–å¸¸è§æ‰©å±•å
    if not parquet_files:
        parquet_files = list(input_path.rglob("*.parq"))
    
    return parquet_files

def process_files_parallel(
    input_dir: str, 
    output_dir: str, 
    max_workers: int = None,
    chunk_size: int = 10000
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†æ‰€æœ‰Parquetæ–‡ä»¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        chunk_size: å¤„ç†å¤§æ–‡ä»¶æ—¶çš„åˆ†å—å¤§å°
        
    Returns:
        æ‰€æœ‰æ–‡ä»¶å¤„ç†ç»“æœçš„åˆ—è¡¨
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰Parquetæ–‡ä»¶
    parquet_files = find_parquet_files(input_dir)
    if not parquet_files:
        logger.warning(f"åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°Parquetæ–‡ä»¶")
        return []
    
    logger.info(f"æ‰¾åˆ° {len(parquet_files)} ä¸ªParquetæ–‡ä»¶")
    
    # å‡†å¤‡æ–‡ä»¶å¤„ç†ä¿¡æ¯
    file_infos = [(str(file), output_dir) for file in parquet_files]
    
    # è®¾ç½®è¿›ç¨‹æ•°
    if max_workers is None:
        # é»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°ï¼Œä½†ä¸è¶…è¿‡æ–‡ä»¶æ•°é‡
        max_workers = min(mp.cpu_count(), len(parquet_files))
    
    logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†æ–‡ä»¶
    results = []
    with mp.Pool(processes=max_workers) as pool:
        # ä½¿ç”¨imap_unorderedè·å–å®æ—¶è¿›åº¦
        for i, result in enumerate(pool.imap_unordered(process_single_file, file_infos)):
            results.append(result)
            logger.info(f"è¿›åº¦: {i + 1}/{len(parquet_files)} æ–‡ä»¶å®Œæˆ")
    
    return results

def generate_summary_report(results: List[Dict[str, Any]]) -> None:
    """
    ç”Ÿæˆå¤„ç†ç»“æœæ‘˜è¦æŠ¥å‘Š
    
    Args:
        results: å¤„ç†ç»“æœåˆ—è¡¨
    """
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    total_files = len(results)
    total_rows = sum(r.get('rows_processed', 0) for r in successful)
    total_time = sum(r.get('processing_time', 0) for r in results)
    avg_time_per_file = total_time / total_files if total_files > 0 else 0
    
    print("\n" + "="*60)
    print("ğŸ“Š PARQUETè½¬JSONLå¤„ç†ç»“æœæ‘˜è¦")
    print("="*60)
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æˆåŠŸå¤„ç†: {len(successful)}")
    print(f"å¤„ç†å¤±è´¥: {len(failed)}")
    print(f"æ€»æ•°æ®è¡Œæ•°: {total_rows:,}")
    print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")
    print(f"å¹³å‡æ¯ä¸ªæ–‡ä»¶è€—æ—¶: {avg_time_per_file:.2f} ç§’")
    print(f"å¹³å‡å¤„ç†é€Ÿåº¦: {total_rows/total_time:.0f} è¡Œ/ç§’" if total_time > 0 else "é€Ÿåº¦: N/A")
    
    if successful:
        print(f"\nâœ… æˆåŠŸæ–‡ä»¶å·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•")
    
    if failed:
        print(f"\nâŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨ ({len(failed)} ä¸ª):")
        for i, fail in enumerate(failed, 1):
            print(f"  {i}. {Path(fail['input_file']).name}: {fail['error']}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¤šè¿›ç¨‹Parquetè½¬JSONLæ‰¹é‡è½¬æ¢å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python parquet_to_jsonl.py /input/parquet/files /output/jsonl/files
  python parquet_to_jsonl.py /input/data /output/result --workers 8
        """
    )
    
    parser.add_argument('input_dir', help='åŒ…å«Parquetæ–‡ä»¶çš„è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('output_dir', help='JSONLæ–‡ä»¶çš„è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--workers', type=int, default=None, 
                       help='æœ€å¤§å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°)')
    parser.add_argument('--log', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(getattr(logging, args.log))
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not Path(args.input_dir).exists():
        logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    logger.info(f"å¼€å§‹å¤„ç†: {args.input_dir} -> {args.output_dir}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    overall_start_time = time.time()
    
    # å¤„ç†æ–‡ä»¶
    results = process_files_parallel(args.input_dir, args.output_dir, args.workers)
    
    # è®¡ç®—æ€»è€—æ—¶
    overall_time = time.time() - overall_start_time
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_summary_report(results)
    print(f"\nğŸ å…¨éƒ¨å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {overall_time:.2f} ç§’")

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ï¼ˆWindowséœ€è¦ï¼ŒLinux/macOSè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹å¼ï¼‰
    if mp.get_start_method(allow_none=True) is None:
        mp.set_start_method('spawn')
    
    main()


"""
python convert_parquet2jsonl.py Nemotron-Post-Training-Dataset-v1/ orig2jsonl  --workers 128
"""
